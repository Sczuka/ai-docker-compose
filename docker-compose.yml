services:
  neo4j:
    image: neo4j:latest
    pull_policy: always
    container_name: neo4j
    restart: unless-stopped
    volumes:
      - ai-neo4j-logs:/logs
      - ai-neo4j-config:/config
      - ai-neo4j-data:/data
      - ai-neo4j-plugins:/plugins
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD}
      DB_USER: neo4j
      DB_PASSWORD: ${DB_PASSWORD}
    ports:
      - "${NEO4J_PORT:-7474}:7474"
      - "${NEO4J_BOLT:-7687}:7687"

  ollama:
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    pull_policy: always
    container_name: ollama
    volumes:
      - ai-ollama:/root/.ollama
    tty: true
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      OLLAMA_MAX_LOADED_MODELS: 3
      OLLAMA_MAX_QUEUE: 1024
      OLLAMA_NUM_PARALLEL: 3
      OLLAMA_FLASH_ATTENTION: 1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    pull_policy: always
    container_name: open-webui
    volumes:
      - ai-open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - "${OPEN_WEBUI_PORT:-8080}:8080"
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      WEBUI_SECRET_KEY:
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

  docling:
    image: quay.io/docling-project/docling-serve
    container_name: docling
    environment:
      DOCLING_SERVE_ENABLE_UI: true
    restart: unless-stopped
    ports:
      - "${DOCLING_PORT:-5001}:5001"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  lightrag:
    image: ghcr.io/hkuds/lightrag:1.4.6
    container_name: lightrag
    restart: unless-stopped
    ports:
      - "${PORT:-9621}:9621"
    volumes:
      - ai-lightrag-rag_storage:/app/data/rag_storage
      - ai-lightrag-inputs:/app/data/inputs
      - ai-lightrag-tiktoken:/app/data/tiktoken
    environment:
      LLM_BINDING: ollama
      LLM_MODEL: mistral-nemo:latest
      LLM_BINDING_API_KEY:
      LLM_BINDING_HOST: http://ollama:11434
      LIGHTRAG_GRAPH_STORAGE: Neo4JStorage
      NEO4J_URI: bolt://neo4j:7687/
      NEO4J_DATABASE: neo4j
      NEO4J_USERNAME: neo4j
      NEO4J_USER: neo4j
      NEO4J_PASSWORD: ${NEO4J_PASSWORD}
      NEO4J_MAX_CONNECTION_POOL_SIZE: 100
      NEO4J_CONNECTION_TIMEOUT: 30
      NEO4J_CONNECTION_ACQUISITION_TIMEOUT: 30
      NEO4J_MAX_TRANSACTION_RETRY_TIME: 30
      NEO4J_MAX_CONNECTION_LIFETIME: 300
      NEO4J_LIVENESS_CHECK_TIMEOUT: 30
      NEO4J_KEEP_ALIVE: true
      ENABLE_RERANK: false
    depends_on:
      - neo4j
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

volumes:
  ai-neo4j-logs: { }
  ai-neo4j-config: { }
  ai-neo4j-data: { }
  ai-neo4j-plugins: { }
  ai-ollama:
    driver: local
    driver_opts:
      type: 'none'
      o: 'bind'
      device: '/srv/data/ai/ollama'
  ai-open-webui:
    driver: local
    driver_opts:
      type: 'none'
      o: 'bind'
      device: '/srv/data/ai/open-webui'
  ai-lightrag-rag_storage: { }
  ai-lightrag-inputs: { }
  ai-lightrag-tiktoken: { }
